{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Train a latent SDE on data from a stochastic Lorenz attractor.\n",
    "\n",
    "Reproduce the toy example in Section 7.2 of https://arxiv.org/pdf/2001.01328.pdf\n",
    "\n",
    "To run this file, first run the following to install extra requirements:\n",
    "pip install fire\n",
    "\n",
    "To run, execute:\n",
    "python -m examples.latent_sde_lorenz\n",
    "\"\"\"\n",
    "import logging\n",
    "import os\n",
    "from typing import Sequence\n",
    "\n",
    "import fire\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import torchsde\n",
    "\n",
    "\n",
    "class LinearScheduler(object):\n",
    "    def __init__(self, iters, maxval=1.0):\n",
    "        self._iters = max(1, iters)\n",
    "        self._val = maxval / self._iters\n",
    "        self._maxval = maxval\n",
    "\n",
    "    def step(self):\n",
    "        self._val = min(self._maxval, self._val + self._maxval / self._iters)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return self._val\n",
    "\n",
    "\n",
    "class StochasticLorenz(object):\n",
    "    \"\"\"Stochastic Lorenz attractor.\n",
    "\n",
    "    Used for simulating ground truth and obtaining noisy data.\n",
    "    Details described in Section 7.2 https://arxiv.org/pdf/2001.01328.pdf\n",
    "    Default a, b from https://openreview.net/pdf?id=HkzRQhR9YX\n",
    "    \"\"\"\n",
    "    noise_type = \"diagonal\"\n",
    "    sde_type = \"ito\"\n",
    "\n",
    "    def __init__(self, a: Sequence = (10., 28., 8 / 3), b: Sequence = (.1, .28, .3)):\n",
    "        super(StochasticLorenz, self).__init__()\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "    def f(self, t, y):\n",
    "        x1, x2, x3 = torch.split(y, split_size_or_sections=(1, 1, 1), dim=1)\n",
    "        a1, a2, a3 = self.a\n",
    "\n",
    "        f1 = a1 * (x2 - x1)\n",
    "        f2 = a2 * x1 - x2 - x1 * x3\n",
    "        f3 = x1 * x2 - a3 * x3\n",
    "        return torch.cat([f1, f2, f3], dim=1)\n",
    "\n",
    "    def g(self, t, y):\n",
    "        x1, x2, x3 = torch.split(y, split_size_or_sections=(1, 1, 1), dim=1)\n",
    "        b1, b2, b3 = self.b\n",
    "\n",
    "        g1 = x1 * b1\n",
    "        g2 = x2 * b2\n",
    "        g3 = x3 * b3\n",
    "        return torch.cat([g1, g2, g3], dim=1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, x0, ts, noise_std, normalize):\n",
    "        \"\"\"Sample data for training. Store data normalization constants if necessary.\"\"\"\n",
    "        xs = torchsde.sdeint(self, x0, ts)\n",
    "        if normalize:\n",
    "            mean, std = torch.mean(xs, dim=(0, 1)), torch.std(xs, dim=(0, 1))\n",
    "            xs.sub_(mean).div_(std).add_(torch.randn_like(xs) * noise_std)\n",
    "        return xs\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_size)\n",
    "        self.lin = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        out, _ = self.gru(inp)\n",
    "        out = self.lin(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class LatentSDE(nn.Module):\n",
    "    sde_type = \"ito\"\n",
    "    noise_type = \"diagonal\"\n",
    "\n",
    "    def __init__(self, data_size, latent_size, context_size, hidden_size):\n",
    "        super(LatentSDE, self).__init__()\n",
    "        # Encoder.\n",
    "        self.encoder = Encoder(input_size=data_size, hidden_size=hidden_size, output_size=context_size)\n",
    "        self.qz0_net = nn.Linear(context_size, latent_size + latent_size)\n",
    "\n",
    "        # Decoder.\n",
    "        self.f_net = nn.Sequential(\n",
    "            nn.Linear(latent_size + context_size, hidden_size),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(hidden_size, latent_size),\n",
    "        )\n",
    "        self.h_net = nn.Sequential(\n",
    "            nn.Linear(latent_size, hidden_size),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(hidden_size, latent_size),\n",
    "        )\n",
    "        # This needs to be an element-wise function for the SDE to satisfy diagonal noise.\n",
    "        self.g_nets = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(1, hidden_size),\n",
    "                    nn.Softplus(),\n",
    "                    nn.Linear(hidden_size, 1),\n",
    "                    nn.Sigmoid()\n",
    "                )\n",
    "                for _ in range(latent_size)\n",
    "            ]\n",
    "        )\n",
    "        self.projector = nn.Linear(latent_size, data_size)\n",
    "\n",
    "        self.pz0_mean = nn.Parameter(torch.zeros(1, latent_size))\n",
    "        self.pz0_logstd = nn.Parameter(torch.zeros(1, latent_size))\n",
    "\n",
    "        self._ctx = None\n",
    "\n",
    "    def contextualize(self, ctx):\n",
    "        self._ctx = ctx  # A tuple of tensors of sizes (T,), (T, batch_size, d).\n",
    "\n",
    "    def f(self, t, y):\n",
    "        ts, ctx = self._ctx\n",
    "        i = min(torch.searchsorted(ts, t, right=True), len(ts) - 1)\n",
    "        return self.f_net(torch.cat((y, ctx[i]), dim=1))\n",
    "\n",
    "    def h(self, t, y):\n",
    "        return self.h_net(y)\n",
    "\n",
    "    def g(self, t, y):  # Diagonal diffusion.\n",
    "        y = torch.split(y, split_size_or_sections=1, dim=1)\n",
    "        out = [g_net_i(y_i) for (g_net_i, y_i) in zip(self.g_nets, y)]\n",
    "        return torch.cat(out, dim=1)\n",
    "\n",
    "    def forward(self, xs, ts, noise_std, adjoint=False, method=\"euler\"):\n",
    "        # Contextualization is only needed for posterior inference.\n",
    "        ctx = self.encoder(torch.flip(xs, dims=(0,)))\n",
    "        ctx = torch.flip(ctx, dims=(0,))\n",
    "        self.contextualize((ts, ctx))\n",
    "\n",
    "        qz0_mean, qz0_logstd = self.qz0_net(ctx[0]).chunk(chunks=2, dim=1)\n",
    "        z0 = qz0_mean + qz0_logstd.exp() * torch.randn_like(qz0_mean)\n",
    "\n",
    "        if adjoint:\n",
    "            # Must use the argument `adjoint_params`, since `ctx` is not part of the input to `f`, `g`, and `h`.\n",
    "            adjoint_params = (\n",
    "                    (ctx,) +\n",
    "                    tuple(self.f_net.parameters()) + tuple(self.g_nets.parameters()) + tuple(self.h_net.parameters())\n",
    "            )\n",
    "            zs, log_ratio = torchsde.sdeint_adjoint(\n",
    "                self, z0, ts, adjoint_params=adjoint_params, dt=1e-2, logqp=True, method=method)\n",
    "        else:\n",
    "            zs, log_ratio = torchsde.sdeint(self, z0, ts, dt=1e-2, logqp=True, method=method)\n",
    "\n",
    "        _xs = self.projector(zs)\n",
    "        xs_dist = Normal(loc=_xs, scale=noise_std)\n",
    "        log_pxs = xs_dist.log_prob(xs).sum(dim=(0, 2)).mean(dim=0)\n",
    "\n",
    "        qz0 = torch.distributions.Normal(loc=qz0_mean, scale=qz0_logstd.exp())\n",
    "        pz0 = torch.distributions.Normal(loc=self.pz0_mean, scale=self.pz0_logstd.exp())\n",
    "        logqp0 = torch.distributions.kl_divergence(qz0, pz0).sum(dim=1).mean(dim=0)\n",
    "        logqp_path = log_ratio.sum(dim=0).mean(dim=0)\n",
    "        return log_pxs, logqp0 + logqp_path\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size, ts, bm=None):\n",
    "        eps = torch.randn(size=(batch_size, *self.pz0_mean.shape[1:]), device=self.pz0_mean.device)\n",
    "        z0 = self.pz0_mean + self.pz0_logstd.exp() * eps\n",
    "        zs = torchsde.sdeint(self, z0, ts, names={'drift': 'h'}, dt=1e-3, bm=bm)\n",
    "        # Most of the times in ML, we don't sample the observation noise for visualization purposes.\n",
    "        _xs = self.projector(zs)\n",
    "        return _xs\n",
    "\n",
    "\n",
    "def make_dataset(t0, t1, batch_size, noise_std, train_dir, device):\n",
    "    data_path = os.path.join(train_dir, 'lorenz_data.pth')\n",
    "    if os.path.exists(data_path):\n",
    "        data_dict = torch.load(data_path)\n",
    "        xs, ts = data_dict['xs'], data_dict['ts']\n",
    "        logging.warning(f'Loaded toy data at: {data_path}')\n",
    "        if xs.shape[1] != batch_size:\n",
    "            raise ValueError(\"Batch size has changed; please delete and regenerate the data.\")\n",
    "        if ts[0] != t0 or ts[-1] != t1:\n",
    "            raise ValueError(\"Times interval [t0, t1] has changed; please delete and regenerate the data.\")\n",
    "    else:\n",
    "        _y0 = torch.randn(batch_size, 3, device=device)\n",
    "        ts = torch.linspace(t0, t1, steps=100, device=device)\n",
    "        xs = StochasticLorenz().sample(_y0, ts, noise_std, normalize=True)\n",
    "\n",
    "        os.makedirs(os.path.dirname(data_path), exist_ok=True)\n",
    "        torch.save({'xs': xs, 'ts': ts}, data_path)\n",
    "        logging.warning(f'Stored toy data at: {data_path}')\n",
    "    return xs, ts\n",
    "\n",
    "\n",
    "def vis(xs, ts, latent_sde, bm_vis, img_path, num_samples=10):\n",
    "    fig = plt.figure(figsize=(20, 9))\n",
    "    gs = gridspec.GridSpec(1, 2)\n",
    "    ax00 = fig.add_subplot(gs[0, 0], projection='3d')\n",
    "    ax01 = fig.add_subplot(gs[0, 1], projection='3d')\n",
    "\n",
    "    # Left plot: data.\n",
    "    z1, z2, z3 = np.split(xs.cpu().numpy(), indices_or_sections=3, axis=-1)\n",
    "    [ax00.plot(z1[:, i, 0], z2[:, i, 0], z3[:, i, 0]) for i in range(num_samples)]\n",
    "    ax00.scatter(z1[0, :num_samples, 0], z2[0, :num_samples, 0], z3[0, :10, 0], marker='x')\n",
    "    ax00.set_yticklabels([])\n",
    "    ax00.set_xticklabels([])\n",
    "    ax00.set_zticklabels([])\n",
    "    ax00.set_xlabel('$z_1$', labelpad=0., fontsize=16)\n",
    "    ax00.set_ylabel('$z_2$', labelpad=.5, fontsize=16)\n",
    "    ax00.set_zlabel('$z_3$', labelpad=0., horizontalalignment='center', fontsize=16)\n",
    "    ax00.set_title('Data', fontsize=20)\n",
    "    xlim = ax00.get_xlim()\n",
    "    ylim = ax00.get_ylim()\n",
    "    zlim = ax00.get_zlim()\n",
    "\n",
    "    # Right plot: samples from learned model.\n",
    "    xs = latent_sde.sample(batch_size=xs.size(1), ts=ts, bm=bm_vis).cpu().numpy()\n",
    "    z1, z2, z3 = np.split(xs, indices_or_sections=3, axis=-1)\n",
    "\n",
    "    [ax01.plot(z1[:, i, 0], z2[:, i, 0], z3[:, i, 0]) for i in range(num_samples)]\n",
    "    ax01.scatter(z1[0, :num_samples, 0], z2[0, :num_samples, 0], z3[0, :10, 0], marker='x')\n",
    "    ax01.set_yticklabels([])\n",
    "    ax01.set_xticklabels([])\n",
    "    ax01.set_zticklabels([])\n",
    "    ax01.set_xlabel('$z_1$', labelpad=0., fontsize=16)\n",
    "    ax01.set_ylabel('$z_2$', labelpad=.5, fontsize=16)\n",
    "    ax01.set_zlabel('$z_3$', labelpad=0., horizontalalignment='center', fontsize=16)\n",
    "    ax01.set_title('Samples', fontsize=20)\n",
    "    ax01.set_xlim(xlim)\n",
    "    ax01.set_ylim(ylim)\n",
    "    ax01.set_zlim(zlim)\n",
    "\n",
    "    plt.savefig(img_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def main(\n",
    "        batch_size=1024,\n",
    "        latent_size=4,\n",
    "        context_size=64,\n",
    "        hidden_size=128,\n",
    "        lr_init=1e-2,\n",
    "        t0=0.,\n",
    "        t1=2.,\n",
    "        lr_gamma=0.997,\n",
    "        num_iters=5000,\n",
    "        kl_anneal_iters=1000,\n",
    "        pause_every=50,\n",
    "        noise_std=0.01,\n",
    "        adjoint=False,\n",
    "        train_dir='./dump/lorenz/',\n",
    "        method=\"euler\",\n",
    "):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    xs, ts = make_dataset(t0=t0, t1=t1, batch_size=batch_size, noise_std=noise_std, train_dir=train_dir, device=device)\n",
    "    latent_sde = LatentSDE(\n",
    "        data_size=3,\n",
    "        latent_size=latent_size,\n",
    "        context_size=context_size,\n",
    "        hidden_size=hidden_size,\n",
    "    ).to(device)\n",
    "    optimizer = optim.Adam(params=latent_sde.parameters(), lr=lr_init)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=lr_gamma)\n",
    "    kl_scheduler = LinearScheduler(iters=kl_anneal_iters)\n",
    "\n",
    "    # Fix the same Brownian motion for visualization.\n",
    "    bm_vis = torchsde.BrownianInterval(\n",
    "        t0=t0, t1=t1, size=(batch_size, latent_size,), device=device, levy_area_approximation=\"space-time\")\n",
    "\n",
    "    for global_step in tqdm.tqdm(range(1, num_iters + 1)):\n",
    "        latent_sde.zero_grad()\n",
    "        log_pxs, log_ratio = latent_sde(xs, ts, noise_std, adjoint, method)\n",
    "        loss = -log_pxs + log_ratio * kl_scheduler.val\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        kl_scheduler.step()\n",
    "\n",
    "        if global_step % pause_every == 0:\n",
    "            lr_now = optimizer.param_groups[0]['lr']\n",
    "            logging.warning(\n",
    "                f'global_step: {global_step:06d}, lr: {lr_now:.5f}, '\n",
    "                f'log_pxs: {log_pxs:.4f}, log_ratio: {log_ratio:.4f} loss: {loss:.4f}, kl_coeff: {kl_scheduler.val:.4f}'\n",
    "            )\n",
    "            img_path = os.path.join(train_dir, f'global_step_{global_step:06d}.pdf')\n",
    "            vis(xs, ts, latent_sde, bm_vis, img_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n",
      "File \u001b[1;32mc:\\Users\\Anwender\\.conda\\envs\\rivapy\\lib\\site-packages\\torch\\__init__.py:465\u001b[0m\n\u001b[0;32m    451\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(textwrap\u001b[38;5;241m.\u001b[39mdedent(\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;124m            Failed to load PyTorch C extensions:\u001b[39m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;124m                It appears that PyTorch has loaded the `torch/_C` folder\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;124m                or by running Python from a different directory.\u001b[39m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;124m'''\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[39;00m\n\u001b[1;32m--> 465\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[43m_C\u001b[49m):\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBase\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    467\u001b[0m         __all__\u001b[38;5;241m.\u001b[39mappend(name)\n",
      "\u001b[1;31mNameError\u001b[0m: name '_C' is not defined"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from typing import Sequence\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from typing import Optional, Union\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import torchsde\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rivapy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
