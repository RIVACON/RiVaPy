{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Librarieres für RiVaPy Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rivapy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float': '{: 0.5f}'.format})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benötigte Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User inputs\n",
    "TransMat = np.matrix(\"\"\"\n",
    "90.81, 8.33, 0.68, 0.06, 0.08, 0.02, 0.01, 0.01;\n",
    "0.70, 90.65, 7.79, 0.64, 0.06, 0.13, 0.02, 0.01;\n",
    "0.09, 2.27, 91.05, 5.52, 0.74, 0.26, 0.01, 0.06;\n",
    "0.02, 0.33, 5.95, 85.93, 5.30, 1.17, 1.12, 0.18;\n",
    "0.03, 0.14, 0.67, 7.73, 80.53, 8.84, 1.00, 1.06;\n",
    "0.01, 0.11, 0.24, 0.43, 6.48, 83.46, 4.07, 5.20;\n",
    "0.21, 0, 0.22, 1.30, 2.38, 11.24, 64.86, 19.79\"\"\")/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = pd.read_excel(\"C:/Users/Anwender/Desktop/Datenmodell_Krediportfoliomodell.xlsx\", \"Positions\")\n",
    "issuer = pd.read_excel(\"C:/Users/Anwender/Desktop/Datenmodell_Krediportfoliomodell.xlsx\", \"Issuer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_issuer = issuer[\"IssuerID\"].nunique()\n",
    "Nsim = 5000 # num sim for CVaR\n",
    "r = 0 # risk free rate\n",
    "t= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nutzung Credit Metrics Modell innerhalb RiVaPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = rivapy.credit.creditMetricsModel(0.2, 10000, TransMat, positions, issuer, r, t, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127980.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.get_portfolio_VaR()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erste Überlegungen Korrelationen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import sys\n",
    "import math\n",
    "from scipy.linalg import sqrtm\n",
    "from random import seed\n",
    "from random import random\n",
    "import plotly.express as px\n",
    "from pandas_datareader import data as pdr\n",
    "from datetime import date\n",
    "import yfinance as yf\n",
    "yf.pdr_override()\n",
    "import rivapy\n",
    "\n",
    "from numpy.linalg import cholesky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdr.get_data_yahoo('LHA','2007-04-02','2022-03-14')[['Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marketDataDAX = pd.read_excel(\"C:/Users/Anwender/Desktop/^GDAXI.xlsx\", \"DAX\").rename(columns={\"Close\" : \"Close_Dax\"})\n",
    "marketDataBASF = pd.read_excel(\"C:/Users/Anwender/Desktop/^GDAXI.xlsx\", \"BASF\").rename(columns={\"Close\" : \"Close_BASF\"})\n",
    "marketDataLHA = pd.read_excel(\"C:/Users/Anwender/Desktop/^GDAXI.xlsx\", \"Lufthansa\").rename(columns={\"Close\" : \"Close_LHA\"})\n",
    "marketDataVW = pd.read_excel(\"C:/Users/Anwender/Desktop/^GDAXI.xlsx\", \"Volkswagen\").rename(columns={\"Close\" : \"Close_VW\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marketDataDAX = marketDataDAX[marketDataDAX[\"Date\"]>='2007-04-02']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedData = marketDataDAX[['Date', 'Close_Dax']].merge(marketDataBASF[['Date', 'Close_BASF']], on='Date', how='left').merge(marketDataLHA[['Date', 'Close_LHA']], on='Date', how='left').merge(marketDataVW[['Date', 'Close_VW']], on='Date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedData = mergedData.drop(['Date'], axis=1)\n",
    "returns = mergedData.pct_change()\n",
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_mat = returns.corr()\n",
    "correlation_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_pairs = correlation_mat.unstack()\n",
    "\n",
    "print(corr_pairs['Close_Dax','Close_BASF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funktionen außerhalb RiVaPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_map = pd.DataFrame({'Rating': [\"AAA\", \"AA\", \"A\", \"BBB\", \"BB\", \"B\", \"CCC\", \"D\"], 'RatingID': [0, 1, 2, 3, 4, 5, 6, 7]})\n",
    "issuer_adj = issuer.merge(rating_map, on = \"Rating\", how = \"left\")\n",
    "positions_adj = positions.merge(issuer_adj[[\"IssuerID\",\"Rating\",\"RatingID\"]], on = \"IssuerID\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergePositionsIssuer(position_data, issuer_data):\n",
    "    rating_map = pd.DataFrame({'Rating': [\"AAA\", \"AA\", \"A\", \"BBB\", \"BB\", \"B\", \"CCC\", \"D\"], 'RatingID': [0, 1, 2, 3, 4, 5, 6, 7]})\n",
    "    issuer_adj = issuer_data.merge(rating_map, on = \"Rating\", how = \"left\")\n",
    "    positions_adj = positions.merge(issuer_adj[[\"IssuerID\",\"Rating\",\"RatingID\"]], on = \"IssuerID\", how = \"left\")\n",
    "\n",
    "    return positions_adj\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation_matrix (rho, n):\n",
    "    sigma = rho*np.ones((n,n))\n",
    "    sigma = sigma -np.diag(np.diag(sigma)) + np.eye(n)\n",
    "    return sigma\n",
    "\n",
    "def get_cutoffs_rating(transition_matrix):\n",
    "    Z=np.cumsum(np.flipud(transition_matrix.T),0)\n",
    "    Z[Z>=(1-1/1e12)] = 1-1/1e12;\n",
    "    Z[Z<=(0+1/1e12)] = 0+1/1e12;\n",
    "\n",
    "    CutOffs=norm.ppf(Z,0,1) # compute cut offes by inverting normal distribution\n",
    "    return(CutOffs)\n",
    "\n",
    "def get_cholesky_decomposition(rho, n):\n",
    "    # simulate jointly normals with sigma as vcov matrix\n",
    "    # use cholesky decomposition\n",
    "\n",
    "    sigma = get_correlation_matrix(rho, n)\n",
    "    c = cholesky(sigma)\n",
    "\n",
    "    return(c)\n",
    "\n",
    "def get_cut_ratings(transition_matrix, index_rating):\n",
    "    \n",
    "    # idx = position_data[\"RatingID\"]\n",
    "    cutOffs = get_cutoffs_rating(transition_matrix)\n",
    "    # cut off matrix for each bond based on their ratings\n",
    "    cut = np.matrix(cutOffs[:,index_rating]).T\n",
    "\n",
    "    return(cut)\n",
    "\n",
    "def get_credit_spreads(transition_matrix, LGD):\n",
    "    # credit spread implied by transmat\n",
    "    PD_t = transition_matrix[:,-1] # default probability at t\n",
    "    credit_spread = -np.log(1-LGD*PD_t)/1\n",
    "    \n",
    "    return(credit_spread)\n",
    "\n",
    "def get_expected_value (r, position_data,  transition_matrix, t):\n",
    "    exposure = np.matrix(position_data[\"Exposure\"]).T\n",
    "    # print(exposure)\n",
    "    idx = position_data[\"RatingID\"]\n",
    "    # print(idx)\n",
    "    LGD = 0.45\n",
    "    credit_spread = get_credit_spreads(transition_matrix, LGD)\n",
    "    # print(credit_spread)\n",
    "    EV = np.multiply(exposure, np.exp(-(r+credit_spread[idx])*t))\n",
    "\n",
    "    return(EV)\n",
    "\n",
    "def get_states (transition_matrix, position_data, r, t):\n",
    "    # bond state variable for security Value\n",
    "    LGD = 0.45\n",
    "    recover = 0.55\n",
    "    credit_spread = get_credit_spreads(transition_matrix, LGD)\n",
    "    cp = np.tile(credit_spread.T,[position_data[\"InstrumentID\"].nunique(),1])\n",
    "    # print(cp)\n",
    "    exposure = np.matrix(position_data[\"Exposure\"]).T\n",
    "    # print(exposure)\n",
    "    state = np.multiply(exposure,np.exp(-(r+cp)*t))\n",
    "    # print(state)\n",
    "    state = np.append(state,np.multiply(exposure,recover),axis=1) #last column is default case\n",
    "    # print(state)\n",
    "    states = np.fliplr(state) # keep in same order as credit cutoff\n",
    "    # print(states)\n",
    "\n",
    "    return(states)\n",
    "\n",
    "def mc_calculation(rho, n_issuer, n_simulation, transition_matrix, position_data, r, t):\n",
    "    # c = get_cholesky_distribution(rho, n_issuer)\n",
    "    # cut = get_cut_ratings(transition_matrix, position_data)\n",
    "    cutOffs = get_cutoffs_rating(transition_matrix)\n",
    "    states = get_states (transition_matrix, position_data, r, t)\n",
    "    EV = get_expected_value (r, position_data,  transition_matrix, t)\n",
    "    n_positions = position_data[\"InstrumentID\"].nunique()\n",
    "    Loss = np.zeros((n_simulation,n_positions))\n",
    "    # np.random.seed(1)\n",
    "\n",
    "    for i in range(0,n_simulation):\n",
    "        YY = norm.ppf(np.random.rand())\n",
    "        # rr=c*YY.T\n",
    "        rr = YY*rho\n",
    "        for j in range (0,n_positions):\n",
    "            YY_ido = norm.ppf(np.random.rand())\n",
    "            #corr_idio=np.sqrt((1-(c*c)))\n",
    "            rr_idio=np.sqrt(1-(rho**2))*YY_ido\n",
    "            # print(rr_idio)\n",
    "            rr_all=rr+rr_idio\n",
    "            # print(rr_all)\n",
    "            rating = np.array(rr_all<np.matrix(cutOffs[:,position_data.loc[j,\"RatingID\"]]).T)\n",
    "            rate_idx = len(rating) - np.sum(rating,0)\n",
    "            # print(rate_idx)\n",
    "            col_idx = rate_idx\n",
    "            V_t = states[j,col_idx] # retrieve the corresponding state value of the exposure\n",
    "            Loss_t = V_t-EV.item(j)\n",
    "            # print(Loss_t)\n",
    "            Loss[i,j] = Loss_t\n",
    "            # print(Loss)\n",
    "\n",
    "    # Portfolio_MC_Loss = np.sum(Loss,1)\n",
    "    return(Loss)\n",
    "\n",
    "def get_Loss_distribution (rho, n_issuer, n_simulation, transition_matrix, position_data, r, t):\n",
    "    Loss = mc_calculation(rho, n_issuer, n_simulation, transition_matrix, position_data, r, t)\n",
    "    Portfolio_MC_Loss = np.sum(Loss,1)\n",
    "\n",
    "    return(Portfolio_MC_Loss)\n",
    "\n",
    "def get_portfolio_VaR(rho, n_issuer, n_simulation, transition_matrix, position_data, r, t, confidencelevel):\n",
    "    loss_Distribution = get_Loss_distribution(rho, n_issuer, n_simulation, transition_matrix, position_data, r, t)\n",
    "    Port_Var = -1*np.percentile(loss_Distribution,confidencelevel)\n",
    "\n",
    "    return(Port_Var)\n",
    "\n",
    "def get_portfolio_ES(rho, n_issuer, n_simulation, transition_matrix, position_data, r, t, confidencelevel):\n",
    "    loss_Distribution = get_Loss_distribution(rho, n_issuer, n_simulation, transition_matrix, position_data, r, t)\n",
    "    portVar = get_portfolio_VaR(rho, n_issuer, n_simulation, transition_matrix, position_data, r, t, confidencelevel)\n",
    "\n",
    "    expectedShortfall = -1*np.mean(loss_Distribution[loss_Distribution<-1*portVar])\n",
    "\n",
    "    return(expectedShortfall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = get_portfolio_VaR(0.2, n_issuer, 5000, TransMat, positions_adj, r, t, 1)\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = get_Loss_distribution(0.2, n_issuer, 5000, TransMat, positions_adj, r, t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = get_portfolio_ES(0.2, n_issuer, 2000, TransMat, positions_adj, r, t, 1)\n",
    "es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossDistribution = mc_calculation(0.2, n_issuer, 20000, TransMat, positions_adj, r, t)\n",
    "test_df = pd.DataFrame(lossDistribution)\n",
    "Port_Var = -1*np.percentile(lossDistribution,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.rename(columns={0: \"Value\"})\n",
    "test_df.groupby([\"Value\"]).size().reset_index(name='Count').sort_values(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(TransMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6adee53c13be7cf8582dc470917a193e541c9e29837d59b0c484fc99790beedb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
