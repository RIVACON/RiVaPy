{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kreditausfallmodellierung mi Hilfe von KI - Wie erklärbar sind denn die Modelle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warum?\n",
    "\n",
    "- blablabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Globalgalaktische Betrachtung von \"Erklärbarkeit\":\n",
    "\n",
    "- Intrinsisch oder „Post hoc“?​ Interpretierbarkeit durch Begrenzung der Komplexität des KI-Modells ​(z.B.  Entscheidungsbäume) oder durch Modellanalysen nach Training?​\n",
    "\n",
    "- Modellspezifisch oder modellunabhängig?​ Modellspezifische Interpretationswerkzeuge sind auf bestimmte Modellklassen ​beschränkt (z.B. Interpretation von Regressionsgewichten in einem linearen Modell). ​Modellunabhängige Werkzeuge können für jedes maschinelle Lernmodell verwendet werden und werden angewendet, nachdem das Modell trainiert wurde (z.B. Analyse von​ Ein- und Ausgabepaaren von Merkmalen). ​\n",
    "\n",
    "- Lokal oder global? ​Erklärt die Interpretationsmethode eine einzelne Vorhersage oder das gesamte  Modellverhalten?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Was?\n",
    "\n",
    "#### Umfang der Interpretierbarkeit:\n",
    "\n",
    "- Transparenz des Algorithmus? Wie erstellt der Algorithmus das Modell?​\n",
    "-- Verständnis dafür, wie der Algorithmus aus Daten lernt und welche Art von Beziehungen  er lernen kann? ​\\\n",
    "-- Methode der kleinsten Quadrate für lineare Modelle: hohe Transparenz, ​\\\n",
    "-- Deep Learning: weniger Transparenz​\n",
    "\n",
    "- Globale, ganzheitliche Modellinterpretierbarkeit. Wie trifft das trainierte Modell Vorhersagen?​\n",
    "-- Verständnis dafür, wie das Modell Entscheidungen trifft, und zwar auf der Grundlage einer ganzheitlichen Betrachtung seiner Merkmale und der einzelnen gelernten Komponenten wie Gewichte, Parameter und Strukturen. ​\\\n",
    "-- Welche Merkmale sind wichtig und welche Art von Interaktion findet zwischen ihnen statt? ​\n",
    "\n",
    "- Globale Modellinterpretierbarkeit auf modularer Ebene. Wie wirken sich Teile des Modells auf die Vorhersagen aus?​\n",
    "-- Verständnis dafür, wie Modelle auf modularer Ebene funktionieren. ​\\\n",
    "-- z.B. können bei linearen Modellen die Gewichte gut interpretiert werden, während bei Bäumen die Splits (ausgewählte Merkmale plus Abschneidepunkte) und die Blattknotenvorhersagen gut interpretierbar sind​\n",
    "\n",
    "- Lokale Interpretierbarkeit für eine einzelne Vorhersage. Warum hat das Modell eine bestimmte Vorhersage für eine Instanz getroffen?​\n",
    "-- Verständnis dafür, was das Modell für eine einzelne Instanz oder Eingabe vorhersagt und erklären warum. ​\n",
    "\n",
    "- Lokale Interpretierbarkeit für eine Gruppe von Vorhersagen. Warum hat das Modell bestimmte Vorhersagen für eine Gruppe von Instanzen gemacht?\n",
    "-- Verständnis dafür, wie Modellvorhersagen für eine Gruppe von Instanzen getroffen werden. ​\n",
    "\n",
    "\n",
    "#### Eigenschaften der Interpretierbarkeit:\n",
    "\n",
    "- Genauigkeit und Zuverlässigkeit: Wie gut sagt eine Erklärung des Modells ungesehene Daten voraus und wie gut kann durch die Erklärung die Vorhersage des Black-Box Modells approximiert werden? ​\n",
    "\n",
    "- Konsistenz: Inwieweit unterscheidet sich eine Erklärung zwischen Modellen, die für dieselbe Aufgabe trainiert wurden und die ähnliche Vorhersagen liefern?​\n",
    "\n",
    "- Stabilität: Wie ähnlich sind die Erklärungen für ähnliche Instanzen? Während die Konsistenz Erklärungen zwischen Modellen vergleicht, vergleicht die Stabilität Erklärungen zwischen ähnlichen Instanzen für ein festes Modell.​\n",
    "\n",
    "- ​Verständlichkeit: Wie gut verstehen Menschen die Erklärungen?​\n",
    "\n",
    "- Gewissheit: Spiegelt die Erklärung die Gewissheit des maschinellen Lernmodells wider? Viele Modelle des maschinellen Lernens geben nur Vorhersagen, ohne eine Aussage über die Sicherheit des Modells, dass die Vorhersage richtig ist.​\n",
    "\n",
    "- Grad der Wichtigkeit: Wie gut spiegelt die Erklärung die Bedeutung von Merkmalen oder Teilen der Erklärung wider?​\n",
    "\n",
    "- Neuheit: Spiegelt die Erklärung wider, ob eine zu erklärende Dateninstanz aus einer \"neuen\" Region stammt, die weit von der Verteilung der Trainingsdaten entfernt ist? In solchen Fällen kann das Modell ungenau und die Erklärung nutzlos sein.​\n",
    "\n",
    "- Repräsentativität: Wie viele Instanzen deckt eine Erklärung ab? Erklärungen können das gesamte Modell abdecken (z. B. die Interpretation von Gewichten in einem linearen Regressionsmodell) oder nur eine einzelne Vorhersage darstellen (z. B. Shapley-Werte).​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wie?\n",
    "\n",
    "- Verwende \"interpretierbare\" Modelle\n",
    "\n",
    "-- z.B. Decision Trees oder Decision Rule (zB Ripper Algorithmus oder Bayesian Rule Lists/Scalable Bayesian Rule Lists​) für Credit Default: \\\n",
    "-> Klassifizierungsproblem \\\n",
    "-> Zusammenhang zwischen Features und Outcome nicht linear \\\n",
    "-> Monotonie in Beziehung Feature und Outcome (erleichtert Verständnis) \\\n",
    "-> Berücksichtige WW zwischen Features \\\n",
    "\n",
    "\n",
    "-- Vorteile Decision Tree: \\\n",
    "gut erklärbar/leicht interpretierbar (solange Baum kurz ist) und visualisierbar; ideal um WW zwischen Features zu modellieren,  muss Features als Input nicht unbedingt transformieren\n",
    "\n",
    "-- Nachteile Decision Tree: \\\n",
    "nicht in der Lage lineare Zusammenhänge abzubilden (approx. über Stufenfunktion); Geringfügige Änderungen des Eingangsmerkmals können große Auswirkungen auf das vorhergesagte Ergebnis haben; instabil, Einige wenige Änderungen im Trainingsdatensatz können einen völlig anderen Baum erzeugen. \n",
    "\n",
    "\n",
    "-- Vorteile Decision Rule: \\\n",
    "einfach interpretierbar, kompakter als Entscheidungsbäume; schnell (binär), robust gegenüber monotonen Transformationen der Eingangsmerkmale, da sich nur der Schwellenwert in den Bedingungen ändert. Sie sind auch robust gegenüber Ausreißern, da es nur darauf ankommt, ob eine Bedingung zutrifft oder nicht. Fokussieren sich auf relevanten Merkmale.\n",
    "\n",
    "-- Nachteile Decision Rule: \\\n",
    "Fokus auf Klassifizierung, Features müssen Kategorisiert werden, Anfällig für Overfitting, schlecht geeignet, um lineare Beziehungen zwischen Merkmalen und Output zu beschreiben\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
